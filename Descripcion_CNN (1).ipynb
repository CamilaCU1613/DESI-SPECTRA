{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af4134f-fd2c-462d-816f-b584f74b54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f592b744-7756-4e6e-af19-c6bb77c71412",
   "metadata": {},
   "source": [
    "Aqui preprocesaremos las etiquetas de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed4a236-4611-4e8c-a4c2-932a502696be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que tienes una serie de tiempo representada como un numpy array 'X' y las etiquetas 'y' o 'z'\n",
    "X=np.array(np.linspace(0.7,1.2,100)) #Para nosotros, la 'serie de tiempo' seran los datos de flujo, X de un solo espectro\n",
    "\n",
    "y=np.linspace(0,5,5)\n",
    "z=np.array([\"a\",\"b\",\"c\",\"d\",\"b\"])\n",
    "\n",
    "# Preprocesar las etiquetas utilizando LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  # Convierte etiquetas de texto a números enteros\n",
    "y_onehot = to_categorical(y_encoded)  # Convierte etiquetas numéricas a one-hot encoding ej: rojo=[1,0,0], azul=[0,1,0], verde=[0,0,1]\n",
    "\n",
    "z_encoded = label_encoder.fit_transform(z)  # Convierte etiquetas de texto a números enteros\n",
    "z_onehot = to_categorical(z_encoded)  # Convierte etiquetas numéricas a one-hot encoding ej: rojo=[1,0,0], azul=[0,1,0], verde=[0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92ed891-3ed0-4c2f-bcf2-3f3257275409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   1.25 2.5  3.75 5.  ]\n",
      "['a' 'b' 'c' 'd' 'b']\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3852d54c-93df-4c1a-bd98-d6c3b68348f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[0 1 2 3 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_encoded)\n",
    "print(z_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dca1ab4a-6f3a-4ed2-abc3-56765dfdccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_onehot)\n",
    "print(z_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff4415-1cf1-4461-9ec3-71005c4455c1",
   "metadata": {},
   "source": [
    "Dividiremos el conjunto de datos en datos de entrenamiento y datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dfab6a9-36ef-418a-9dcb-dcaa964128c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "y=np.random.randint(0,6,100) #Son las etiquetas asignadas a cada punto de la lista de flujo\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89339d35-3721-4502-9584-1257ba3ae7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[0.7        0.70505051 0.71010101 0.71515152 0.72020202 0.72525253\n",
      " 0.73030303 0.73535354 0.74040404 0.74545455 0.75050505 0.75555556\n",
      " 0.76060606 0.76565657 0.77070707 0.77575758 0.78080808 0.78585859\n",
      " 0.79090909 0.7959596  0.8010101  0.80606061 0.81111111 0.81616162\n",
      " 0.82121212 0.82626263 0.83131313 0.83636364 0.84141414 0.84646465\n",
      " 0.85151515 0.85656566 0.86161616 0.86666667 0.87171717 0.87676768\n",
      " 0.88181818 0.88686869 0.89191919 0.8969697  0.9020202  0.90707071\n",
      " 0.91212121 0.91717172 0.92222222 0.92727273 0.93232323 0.93737374\n",
      " 0.94242424 0.94747475 0.95252525 0.95757576 0.96262626 0.96767677\n",
      " 0.97272727 0.97777778 0.98282828 0.98787879 0.99292929 0.9979798\n",
      " 1.0030303  1.00808081 1.01313131 1.01818182 1.02323232 1.02828283\n",
      " 1.03333333 1.03838384 1.04343434 1.04848485 1.05353535 1.05858586\n",
      " 1.06363636 1.06868687 1.07373737 1.07878788 1.08383838 1.08888889\n",
      " 1.09393939 1.0989899  1.1040404  1.10909091 1.11414141 1.11919192\n",
      " 1.12424242 1.12929293 1.13434343 1.13939394 1.14444444 1.14949495\n",
      " 1.15454545 1.15959596 1.16464646 1.16969697 1.17474747 1.17979798\n",
      " 1.18484848 1.18989899 1.19494949 1.2       ]\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7814228-c34c-4fbe-9929-c735916cc3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "[0.97777778 1.14444444 0.83131313 0.91212121 1.04848485 0.77575758\n",
      " 0.9020202  1.18484848 0.74545455 1.06363636 0.75555556 0.93737374\n",
      " 1.12929293 0.84141414 1.16969697 0.72525253 1.03333333 1.02828283\n",
      " 0.87676768 0.78080808 0.94747475 0.87171717 0.73535354 1.17979798\n",
      " 0.83636364 0.7959596  1.10909091 0.82626263 1.01313131 0.76565657\n",
      " 0.82121212 0.71515152 0.78585859 0.89191919 0.74040404 1.09393939\n",
      " 0.73030303 1.02323232 0.88181818 1.14949495 0.98282828 1.2\n",
      " 0.97272727 0.91717172 0.95252525 1.03838384 0.93232323 1.04343434\n",
      " 1.00808081 1.18989899 1.0989899  0.90707071 0.99292929 0.94242424\n",
      " 1.19494949 0.98787879 1.07878788 0.86161616 1.17474747 0.9979798\n",
      " 1.01818182 1.12424242 0.88686869 0.84646465 0.70505051 0.96262626\n",
      " 0.80606061 0.71010101 0.81616162 1.13939394 1.15959596 1.07373737\n",
      " 1.13434343 1.11414141 0.8010101  1.0030303  1.05858586 0.77070707\n",
      " 1.16464646 0.95757576]\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d660353b-8d4d-45a4-830d-0f362949a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[1.11919192 0.96767677 1.05353535 0.92727273 0.92222222 0.8969697\n",
      " 0.81111111 1.1040404  0.75050505 0.7        0.79090909 0.85151515\n",
      " 1.06868687 0.86666667 1.15454545 0.72020202 1.08383838 1.08888889\n",
      " 0.76060606 0.85656566]\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02405a-edb3-4e10-a90e-42d221271872",
   "metadata": {},
   "source": [
    "Definir el modelo de la red convolusional"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb91365b-4362-4eb6-b8a3-1646b987bb69",
   "metadata": {},
   "source": [
    "Aqui las mejores opciones seria un modelo de Redes Residuales(ResNet) o Redes Convolucionales 1D. Escogemos la segunda por las siguientes razones:\n",
    "\n",
    "Naturaleza Unidimensional de los Datos: Los datos espectrales son inherentemente unidimensionales, ya que se representan como una serie de puntos de datos a lo largo de la longitud de onda. Las CNN 1D están diseñadas para manejar datos unidimensionales y pueden capturar patrones y características en esta dimensión de manera efectiva.\n",
    "\n",
    "Eficiencia Computacional: Dado que estás trabajando con datos unidimensionales, las CNN 1D son más eficientes en términos computacionales en comparación con las redes ResNet, que están diseñadas para imágenes bidimensionales. Esto puede ser importante cuando se trabaja con un gran volumen de datos.\n",
    "\n",
    "Capacidad de Aprender Características Locales: Las CNN 1D pueden aprender características locales en los espectros, lo que puede ser beneficioso para detectar patrones espectrales específicos en diferentes regiones de longitud de onda.\n",
    "\n",
    "Menor Profundidad: Las CNN 1D generalmente requieren menos capas en comparación con las redes ResNet, lo que puede facilitar el entrenamiento y evitar problemas de sobreajuste cuando se tienen datos limitados.\n",
    "\n",
    "Facilidad de Implementación: Implementar y ajustar una CNN 1D es más simple y puede requerir menos recursos computacionales en comparación con arquitecturas más complejas como ResNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e51920f-5a6c-47bd-b620-9cdbecaa1c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lLamamos el modelo CNN1D asi:\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb62ebc-bbcd-4a39-bb32-22cbaffa1f13",
   "metadata": {},
   "source": [
    "Agregamos una capa convolusional 1D"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5016804a-d696-4ac4-8c36-41bb04632cb8",
   "metadata": {},
   "source": [
    "Filters: Establece la cantidad de filtros (mascaras) que se aplicaran sobre los datos de entrada, el valor 32 es tipico y se puede ajsutar pero teniendo en cuenta que una mayor cantidad de estos implica una mayor complejidad.\n",
    "\n",
    "Kernel_size: Establece el tamaño o la dimension de los filtros que utilizaremos. En este caso, el filtro de movera de tres en tres puntos haciando convolusion sobre estos para todos los datos de entrada.\n",
    "\n",
    "Activation: Estas funciones buscan introducir una no linealidad en el modelo para poder extraer información con mayor eficiencia. Por la naturaleza de los datos de entrada, se sugiere utilizar alguna de estas dos funciones de activacion comunes:\n",
    "        -ReLU (Rectified Linear Unit):\n",
    "            Función: ReLU(x) = max(0, x)\n",
    "            Ventajas:\n",
    "                Introduce no linealidad en el modelo, lo que permite capturar relaciones no lineales en los datos.\n",
    "                Es computacionalmente eficiente y acelera el entrenamiento de la red.\n",
    "        \n",
    "        -Tangente Hiperbólica (tanh):\n",
    "            Función: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "            Ventajas:\n",
    "                Es otra función no lineal que puede capturar relaciones no lineales en los datos.\n",
    "                Tiene una salida centrada en cero, lo que puede ser útil para problemas de regresión donde se espera una respuesta tanto positiva como                  negativa.\n",
    "        -Consideraciones:Ambas pueden sufrir el problema de \"neuronas muertas\". Sin embargo, tanh satura valores en -1 y 1 por lo que puede llegar a             relentizar el proceso de entrenamiento.\n",
    "\n",
    "\n",
    "Input_shape: El parámetro input_shape especifica la forma de los datos de entrada. En este caso, (longitud_de_onda, 1) indica que cada secuencia de entrada tiene una longitud de longitud_de_onda y una sola característica por punto de datos. Esto es apropiado para datos espectrales unidimensionales, donde cada punto de datos representa la intensidad de luz en una longitud de onda específica. La dimensión 1 se refiere a la profundidad de la entrada, que es típicamente 1 para datos unidimensionales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2cb3129-881e-4cac-b31b-a51628123fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=((X_train.shape[:1][0],1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5936140e-82ec-4ebe-b9b1-a4551d245b42",
   "metadata": {},
   "source": [
    "Agregamos una capa de max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f99538-739c-465a-b734-82f6dab5efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling1D(pool_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae317107-e079-44cd-b79e-b9280e8ff66e",
   "metadata": {},
   "source": [
    "Aplanamos la salida para conectarla a capas densas"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35e731c1-c7ce-41ec-aa40-83b8b0306beb",
   "metadata": {},
   "source": [
    "Las capas densas que crearemos más adelante requieren una entrada unidimensional. Cuando aplicamos los 32 filtros, nos quedamos con 32 \"listas\" cuya dimension es la de la entrada. Al aplicar max pooling solo reducimos la dimensionalidad de cada una de las 32 \"listas\" pero aun asi, ahora tenemos una entrada multidimensional.\n",
    "En 1D se pondran cada una de las 32 listas una al lado de la otra con el fin de obtener una unica entrada de 1 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d141f754-cc83-4342-bc9b-04d52d60fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6acfa91-2f78-4417-adc9-7698d5b45955",
   "metadata": {},
   "source": [
    "Agregamos capas densas"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f7e3054-6092-454e-b72f-adc01d0aa66c",
   "metadata": {},
   "source": [
    "Todas las entradas de la capa anterior se conectan a toodas las neuronas de cada una de las capas densas.\n",
    "El 128 que significa que en la primera capa hay 128 neuronas. A mayor cantidad de neuronas, se pueden capturar caracteristicas más complejas, pero aumenta la complejidad del modelo.\n",
    "\n",
    "La funcion de activación define como se entregaran los datos a la siguiente capa densa. Nuevamente seria conveniente elegir entre Relu y Tanh para ejercicios de clasificación o regresion.\n",
    "\n",
    "Finalmente, para la capa de salida al final de todas las capas densas todo depende del tipo de ejercicio que se este haciendo. Para clasificación se utiliza la función softmax la cual asigna una probabilidad entre 0 y 1 con el fin de determinar si es más probable que el objeto de entrada sea una estrella, QSO o galaxia. Debido al formato one-hot encoding devolvera algo como, por ejemplo: Galaxia=[1,0,0], Estrella=[0,1,0], QSO=[0,0,1]. Aunque en realidad retornara algo como [0.2,0.7,0.1] lo que significa que hay un 20% de probabilidad de que sea galaxia, 70% estrella y 10% QSO. El numero de neuronas que se imponen en esta capa de salida es exactamente la cantidad de clases que estamos tratando, en este ejemplo 3.\n",
    "\n",
    "Por otro lado, para regresion, se recomienda utilizar la funcion de activación 'linear'. Esta capa posee una unica neurona, ya que su objetivo es retornar un unico valor numerico que corresponde a la predicción del corrimiento al rojo. Entonces 'linear' retorna una combinacion lineal de los valores de las entradas y es altamente recomendable para el calculo de cantidades numericas continuas, como el redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5971a5f-7b8a-495f-bc52-3c78e0d86d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "#model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Capa de salida con activación softmax para clasificación\n",
    "model.add(Dense(units=1, activation='linear'))  # Capa de salida lineal para regresiónn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5677cd5-5065-47d0-8b62-b8e9029db60a",
   "metadata": {},
   "source": [
    "Compilamos el modelo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0455234e-de42-4e2a-ace4-fc7432a9e567",
   "metadata": {},
   "source": [
    "El optimizador es quien se encarga del 'Back propagation' y ajusta los pesos de cada una de las neuronas de la CNN. 'adam' es bastante popular y sera el que utilizaremos en primera instancia. Sin embargo, es conveniente mencionar que existen más optimizadores como SGD, RMSprop, Nadam y Adagrad. Algunos de ellos dependen en gran manera de la tasa de aprendizaje.\n",
    "\n",
    "Loss es la funcion de coste. Esta mide la discrepancia entre las predicciones del modelo y los valores reales (etiquetas) y se utiliza para guiar el proceso de optimización:\n",
    "\n",
    "    Para este tipo de clasificación se sugiere:\n",
    "    -'Categorical_crossentropy' se utiliza típicamente cuando estás realizando una tarea de clasificación multiclase y tus etiquetas están\n",
    "      codificadas en one-hot encoding. Esta función de pérdida es apropiada cuando tienes varias clases y deseas minimizar la divergencia entre las\n",
    "      distribuciones de probabilidad de las predicciones y las etiquetas reales.\n",
    "    -Binary Crossentropy ('binary_crossentropy'): Si estás realizando una clasificación binaria en la que cada muestra pertenece a una de dos clases,\n",
    "      esta función de pérdida es apropiada. Es especialmente útil cuando tus etiquetas son binarias (0 o 1).\n",
    "\n",
    "    Para el problema de regresión se sugiere:\n",
    "    -Mean Squared Error ('mean_squared_error'): Si estás abordando un problema de regresión en lugar de clasificación, puedes utilizar esta función de \n",
    "      pérdida. Mide la discrepancia cuadrática media entre las predicciones y los valores reales. Es adecuada cuando tu objetivo es predecir un valor \n",
    "      numérico continuo, como en el caso del corrimiento al rojo.\n",
    "    -Error absoluto medio (Mean Absolute Error, MAE): Calcula la media de las diferencias absolutas entre las predicciones y los valores reales. Es \n",
    "      menos sensible a valores atípicos en comparación con el Error Cuadrático Medio.\n",
    "    -Error de Huber: Es una función de pérdida que combina características del Error Cuadrático Medio y el Error Absoluto Medio. Es robusta ante \n",
    "      valores atípicos y puede proporcionar un equilibrio entre las dos métricas anteriores.\n",
    "\n",
    "Finalmente, el parámetro metrics especifica las métricas que se utilizarán para evaluar el rendimiento del modelo durante el entrenamiento y la evaluación. En este caso, se utiliza 'accuracy' (precisión) como métrica para el ejercicio de clasificación. La precisión mide la fracción de muestras clasificadas correctamente. Es una métrica común para problemas de clasificación y proporciona una idea de qué tan bien está funcionando el modelo en términos de clasificación correcta. Por otro lado, para el ejercicio de regresion se recomienda utilizar Mean Absolute Error (MAE): metrics=['mae'] o Mean Squared Error (MSE): metrics=['mse']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c5be8ac-9057-47c1-9775-9acc0f54268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #para problema de clasificación\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])  # Puedes ajustar el optimizador y la función de pérdida según tu tarea. Para regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35ed62-68cb-4d55-a147-8c8542ee0e45",
   "metadata": {},
   "source": [
    "Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7ecb082-f442-4cbd-a8b1-c5e916b3636a",
   "metadata": {},
   "source": [
    "\n",
    "epochs: son el numero de iteraciones completas a través del conjunto de datos de entrenamiento. Una época es una pasada completa a través de todos los ejemplos de entrenamiento.\n",
    "\n",
    "batch_size: Este parámetro determina el tamaño del lote (batch) de ejemplos de entrenamiento que se utiliza en cada iteración durante el entrenamiento. Los modelos de aprendizaje profundo suelen entrenarse en mini lotes (mini-batches) en lugar de usar todo el conjunto de datos a la vez. En este caso, se utiliza un tamaño de lote de 32, lo que significa que se toman 32 ejemplos de entrenamiento a la vez en cada iteración.\n",
    "\n",
    "validation_split: Este parámetro se utiliza para especificar una fracción del conjunto de entrenamiento que se utilizará como conjunto de validación durante el entrenamiento. En este caso, se establece en 0.2, lo que significa que el 20% de los datos de entrenamiento se utilizarán como conjunto de validación. Esto es útil para monitorear el rendimiento del modelo en datos que no se utilizan en el entrenamiento y detectar posibles problemas de sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20cf8d30-9317-4566-a29d-8859c81278a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"conv1d\" is incompatible with the layer: expected min_ndim=3, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2) #el validation_split debe coincidir con el test_size del preprocesamiento.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filea36evyni.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\users\\daniel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"conv1d\" is incompatible with the layer: expected min_ndim=3, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2) #el validation_split debe coincidir con el test_size del preprocesamiento.\n",
    "history= model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32adea24-055e-47d3-a7e1-1c3d359864b6",
   "metadata": {},
   "source": [
    "Evaluamos el modelo en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bacc6e6-9c33-4f47-9253-b2ec6506aa47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "loss, accuracy=score[0], score[1]\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(\"Loss:\", score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8300c7-9fef-46c0-ba79-89e4b5af8b65",
   "metadata": {},
   "source": [
    "Visualización del Rendimiento"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed84b602-4d4d-4025-9c7e-b6db99678153",
   "metadata": {},
   "source": [
    "Podemos utilizar la información de historial (history) del entrenamiento para visualizar las curvas de pérdida y precisión en conjuntos de entrenamiento y validación a lo largo de las épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4acd2057-77d2-4f1b-93b8-6ba5c0cd6373",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2998e775-7c41-480c-9da4-770e5a3a82d7",
   "metadata": {},
   "source": [
    "Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75efba72-c5ec-4024-bcbb-5fedf143f989",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_new_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX_new_data\u001b[49m) \u001b[38;5;66;03m#X_new_data son los arrays de espectros sobre los que queremos hacer predicciones\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_new_data' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
