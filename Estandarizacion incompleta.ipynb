{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b53cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from astropy.io import fits\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.table import Table\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from astropy.utils.data import get_pkg_data_filename\n",
    "from astropy.convolution import convolve, Gaussian1DKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e51af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://data.desi.lbl.gov/public/edr/spectro/redux/fuji/healpix/sv3/dark/\"\n",
    "\n",
    "# Obtengo las carpetas y archivos FITS desde la página HTML\n",
    "def get_folders_and_fits(url):\n",
    "    response = requests.get(url)\n",
    "    # Verifica si la respuesta falló y muestra un mensaje\n",
    "    if response.status_code != 200:\n",
    "        print(\"Verifica tu conexión a Internet para asegurarte de que esté funcionando correctamente.\")\n",
    "        return [], []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    folders = []\n",
    "    fits_files = []\n",
    "\n",
    "    # Busco todos los enlaces en la página\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        text = link.text\n",
    "        if href.endswith('/') and not text.startswith('..') and not href.endswith('logs/'):\n",
    "            # Es una carpeta\n",
    "            folder_name = text.rstrip('/')\n",
    "            folders.append(folder_name)\n",
    "        elif text.startswith('redrock-sv3-dark-') or text.startswith('coadd-sv3-dark-'):\n",
    "            # Es un archivo FITS\n",
    "            fits_files.append(text)\n",
    "    \n",
    "    return folders, fits_files\n",
    "\n",
    "# Explorar los archivos de la ultima, tercera, capa. Aqui estan los .fits\n",
    "def explore_third_layer_folders(url):\n",
    "    _ , fits_files = get_folders_and_fits(url)\n",
    "    nstype=[]\n",
    "    # FITS coadd y redrock\n",
    "    for fits_file in tqdm(fits_files):\n",
    "        with open('urlspectra.txt', 'a') as archivo:\n",
    "            if fits_file.startswith('redrock-sv3-dark-'): \n",
    "                red_rock_fits_urls = url + fits_file  #aqui obtengo todos los urls que me llevan a todos los fits de redrock\n",
    "                archivo.write(red_rock_fits_urls+\"\\n\")\n",
    "            if fits_file.startswith('coadd-sv3-dark-'): \n",
    "                coadd_fits_urls = url + fits_file\n",
    "                archivo.write(coadd_fits_urls+\"\\n\")\n",
    "        print(\"copiado\")\n",
    "            \n",
    "\n",
    "    \n",
    "# Explorar las carpetas de la segunda capa 10016/ 10145/ etc/\n",
    "def explore_second_layer_folders(url):\n",
    "    folders, _ = get_folders_and_fits(url)\n",
    "    for folder in folders:\n",
    "        folder_url = url + folder + '/'\n",
    "        explore_third_layer_folders(folder_url)                \n",
    "                \n",
    "# Explora las carpetas de la primera capa: 100/ 101/ etc/\n",
    "def explore_first_layer_folders(url):\n",
    "    folders, _ = get_folders_and_fits(url)\n",
    "    for folder in folders:\n",
    "        folder_url = url + folder + '/'\n",
    "        explore_second_layer_folders(folder_url)\n",
    "        \n",
    "        \n",
    "# Comienzo la exploración desde la URL base en la primera capa\n",
    "explore_first_layer_folders(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('urlspectra.txt', 'r') as archivo:\n",
    "    lineas = archivo.readlines()  # Lee todas las líneas del archivo y las almacena en una lista\n",
    "    for i in range(0, 7, 2): #Este rango va de el primero al tercer fits es decir los primeros 6 fits con 3 readrock y 3 coadd\n",
    "        # Imprime dos líneas a la vez (par de líneas)\n",
    "        if i + 1 < len(lineas):\n",
    "            coadd=fits.open(lineas[i])\n",
    "            readrock=fits.open(lineas[i + 1])\n",
    "            read_readrock=Table.read(readrock, hdu=1)\n",
    "            read_coadd = Table.read(coadd, hdu=1) \n",
    "            targetID_coadd=read_coadd['TARGETID'].data #Target ID lista\n",
    "            Z=read_readrock['Z'].data                  # Redshift lista\n",
    "            spectype=read_readrock['SPECTYPE'].data    # Tipo espectral lista\n",
    "            #obtenemos los datos de flujo de todos los espectros para todos los fits de coadd\n",
    "            Bflux_ns=coadd[4].data\n",
    "            Rflux_ns=coadd[9].data\n",
    "            Zflux_ns=coadd[14].data\n",
    "            Cflux=[]\n",
    "            spectracomplete=[]\n",
    "            Bflux=[]\n",
    "            Rflux=[]\n",
    "            Zflux=[]\n",
    "            for j in range(len(Bflux_ns)): # Estandarización de galaxias y QSO \n",
    "                spectracomplete = np.concatenate((Bflux_ns[j], Rflux_ns[j], Zflux_ns[j]))\n",
    "                datos=spectracomplete\n",
    "                # Paso 1: Calcular la media\n",
    "                media = np.mean(datos)\n",
    "                # Paso 2: Calcular la desviación estándar\n",
    "                desviacion_estandar = np.std(datos) \n",
    "                # Paso 3: Aplicar la estandarización\n",
    "                if desviacion_estandar!=0.0:\n",
    "                    datos_estandarizados = (datos - media) / desviacion_estandar\n",
    "                    Cflux.append(datos_estandarizados)\n",
    "            for m in range(len(Cflux)):  #divide las listas ya estandarizadas por filtros de nuevo \n",
    "                lista_concatenada=Cflux[m]\n",
    "                longitud_Bflux_ns = len(Bflux_ns[m])\n",
    "                longitud_Rflux_ns = len(Rflux_ns[m])\n",
    "                Bsflux = lista_concatenada[:longitud_Bflux_ns]\n",
    "                Rsflux= lista_concatenada[longitud_Bflux_ns:longitud_Bflux_ns + longitud_Rflux_ns]\n",
    "                Zsflux = lista_concatenada[longitud_Bflux_ns + longitud_Rflux_ns:]\n",
    "                Bflux.append(Bsflux)\n",
    "                Rflux.append(Rsflux)\n",
    "                Zflux.append(Zsflux)\n",
    "                \n",
    "                \n",
    "#Faltan las estrellas?\n",
    "                \n",
    "#Creación del archivo fits no se modifico nada \n",
    "\n",
    "\n",
    "            #Ahora, concatenamos todos los flujos (B,V,R) para cada espectro de todos los fits de coadd\n",
    "            Lista = []\n",
    "            #Datos completos de flujo lista de listas\n",
    "            lista_cb = [','.join(map(str, arr)) for arr in Bflux]\n",
    "            lista_cr = [','.join(map(str, arr)) for arr in Rflux]\n",
    "            lista_cz = [','.join(map(str, arr)) for arr in Zflux]\n",
    "            # Define los tipos de datos\n",
    "            dtype = [('BFlux', 'S100000'), ('RFlux', 'S100000'),('ZFlux', 'S100000'), ('Spectype', 'S10'), ('Z', 'float'), ('Targetid', 'int')]\n",
    "            # Crea un arreglo estructurado de NumPy\n",
    "            data = np.zeros(len(lista_cb), dtype=dtype)\n",
    "            data['BFlux'] = lista_cb\n",
    "            data['RFlux'] = lista_cr\n",
    "            data['ZFlux'] = lista_cz\n",
    "            data['Spectype'] = spectype\n",
    "            data['Z'] = Z\n",
    "            data['Targetid'] = targetID_coadd\n",
    "            nombre_archivo = 'DataDESI.fits'\n",
    "            hdulist = fits.open(nombre_archivo, mode='append')\n",
    "            # Crea una tabla FITS usando fits.BinTableHDU\n",
    "            hdu = fits.BinTableHDU.from_columns(data)\n",
    "            hdulist.append(hdu)\n",
    "            hdulist.close()          \n",
    "              \n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a0eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "espc = fits.open('DataDESI.fits')\n",
    "Tabla_fit1 = Table.read(espc, hdu=1)\n",
    "print (Tabla_fit1['RFlux'][-5])\n",
    "#Tabla_fit2 = Table.read(espc, hdu=2)#De esta tabla podemos extraer los TARGET ID y validar el sepctype en otro catalogo\n",
    "#print (len(Tabla_fit2['BFlux'][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
